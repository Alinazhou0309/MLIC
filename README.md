### Update

Release the code of MLIC <sup> ++ </sup> ! 

We highlight MLIC <sup> ++ </sup>, which **sloves the quadratic complexity of global context capturing**!

*MLIC: Multi-Reference Entropy Model for Learned Image Compression* [[Arxiv](https://arxiv.org/abs/2211.07273)] is accepted at ACMMM 2023 !

*MLIC <sup> ++ </sup>: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression*  [[Arxiv](https://arxiv.org/abs/2307.15421)] [[OpenReview](https://openreview.net/forum?id=hxIpcSoz2t)] is accepted at ICML 2023 Neural Compression Workshop !


## Architectures

<img src="./assets/mlic++_arch.png" style="zoom: 33%">


## Performance




```
@article{jiang2022mlic,
  title={Multi-reference entropy model for learned image compression},
  author={Jiang, Wei and Yang, Jiayu and Zhai, Yongqi and Wang, Ronggang},
  journal={arXiv preprint arXiv:2211.07273},
  year={2022}
}
```

```
@article{jiang2023mlic,
  title={MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression}, 
  author={Jiang, Wei and Wang, Ronggang},
  journal={arXiv preprint arXiv:2307.15421},
  year={2023},
}
```
